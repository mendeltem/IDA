{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "The goal of this lab is to introduce you to data preprocessing techniques in order to make your data suitable for applying a learning algorithm.\n",
    "\n",
    "## 1. Handling Missing Values\n",
    "\n",
    "A common (and very unfortunate) data property is the ocurrence of missing and erroneous values in multiple features in our dataset.\n",
    "Download the dataset and corresponding information from the <a href=\"http://www.cs.uni-potsdam.de/ml/teaching/ss15/ida/uebung02/abalone.csv\">course website</a>.\n",
    "\n",
    "To determine the age of a abalone snail you have to kill the snail and count the annual\n",
    "rings. You are told to estimate the age of a snail on the basis of the following attributes:\n",
    "1. type: male (0), female (1) and infant (2)\n",
    "2. length in mm\n",
    "3. width in mm\n",
    "4. height in mm\n",
    "5. total weight in grams\n",
    "6. weight of the meat in grams\n",
    "7. drained weight in grams\n",
    "8. weight of the shell in grams\n",
    "9. number of annual rings (number of rings +1, 5 yields age)\n",
    "\n",
    "However, these data is incomplete. Missing values are marked with −1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>total_weight</th>\n",
       "      <th>meat_weight</th>\n",
       "      <th>drained_weight</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>num_rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.120</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  length  width  height  total_weight  meat_weight  drained_weight  \\\n",
       "0     0   0.350  0.265   0.090        0.2255       0.0995          0.0485   \n",
       "1     1   0.530  0.420   0.135        0.6770       0.2565          0.1415   \n",
       "2     0   0.440  0.365   0.125        0.5160       0.2155          0.1140   \n",
       "3     2  -1.000  0.255   0.080        0.2050       0.0895          0.0395   \n",
       "4     2   0.425  0.300   0.095        0.3515       0.1410          0.0775   \n",
       "\n",
       "   shell_weight  num_rings  \n",
       "0         0.070         -1  \n",
       "1         0.210          9  \n",
       "2         0.155         10  \n",
       "3         0.055          7  \n",
       "4         0.120          8  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load data \n",
    "df = pd.read_csv(\"abalone.csv\")\n",
    "df.columns=['type','length','width','height','total_weight','meat_weight','drained_weight','shell_weight','num_rings']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "\n",
    "Compute the mean of all positive numbers of each numeric column and the counts of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of all positive numbers of each numeric column\n",
      "\n",
      "length            0.523692\n",
      "width             0.407955\n",
      "height            0.139679\n",
      "total_weight      0.828843\n",
      "meat_weight       0.359263\n",
      "drained_weight    0.180249\n",
      "shell_weight      0.238604\n",
      "dtype: float64 \n",
      "\n",
      "The counts of each category\n",
      "\n",
      "      type\n",
      "type      \n",
      "-1      87\n",
      " 0    1500\n",
      " 1    1279\n",
      " 2    1310\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean of all positive numbers of each numeric column\\n\")\n",
    "print(df[df.iloc[:,1:8] > 0].mean()[1:8],\"\\n\")\n",
    "print(\"The counts of each category\\n\")\n",
    "print(df.groupby(['type'])[['type']].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "\n",
    "Compute the median of all positive numbers of each numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of all positive numbers of each numeric column\n",
      "\n",
      "length            0.54500\n",
      "width             0.42500\n",
      "height            0.14000\n",
      "total_weight      0.80175\n",
      "meat_weight       0.33600\n",
      "drained_weight    0.17050\n",
      "shell_weight      0.23350\n",
      "dtype: float64 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pandoora/anaconda3/envs/mi/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:908: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanmedian1d, axis, a, overwrite_input)\n"
     ]
    }
   ],
   "source": [
    "print(\"Median of all positive numbers of each numeric column\\n\")\n",
    "print(df[df.iloc[:,1:8] > 0].median()[1:8],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "\n",
    "Handle the missing values in a way that you find suitable. Argue your choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transorm to numpy array\n",
    "#data\n",
    "X = df.loc[df[\"type\"] >= 0,\"length\":].values\n",
    "#target\n",
    "y = df.loc[df[\"type\"] >= 0,\"type\"].values\n",
    "X_missing = df.loc[df[\"type\"] == -1,\"length\":].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about 2.08 % the data is missing\n"
     ]
    }
   ],
   "source": [
    "print(\"about\",np.round(X_missing.shape[0] / df.shape[0] * 100, decimals = 2),\"% the data is missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dropped it because first it only missing 2% and other classifier cant predict significantlty the missing gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[df[\"type\"] < 0,\"type\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data for modeling\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1, random_state=42, stratify=y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.60      0.53       150\n",
      "           1       0.44      0.31      0.37       128\n",
      "           2       0.74      0.73      0.73       131\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       409\n",
      "   macro avg       0.55      0.55      0.54       409\n",
      "weighted avg       0.55      0.55      0.54       409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#KneighborClassifiert\n",
    "knn = KNeighborsClassifier(n_neighbors=14)\n",
    "knn.fit(X_train,y_train)\n",
    "#print(knn.score(X_test, y_test))\n",
    "y_predict = knn.predict(X_test)\n",
    "print(\"Confusion Matrix\")\n",
    "print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.65      0.52       150\n",
      "           1       0.11      0.02      0.03       128\n",
      "           2       0.60      0.78      0.68       131\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       409\n",
      "   macro avg       0.38      0.48      0.41       409\n",
      "weighted avg       0.39      0.49      0.42       409\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pandoora/anaconda3/envs/mi/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/pandoora/anaconda3/envs/mi/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_predict = logreg.predict(X_test)\n",
    "print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if those ware predictable:\n",
    "#y_filling = model.predict(X_missing)\n",
    "#cleaned_df = pd.DataFrame(data= np.c_[y_filling, X_missing],columns = df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4\n",
    "\n",
    "Perform Z-score normalization on every column (except the type of course!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean  length            0.478210\n",
      "width             0.366149\n",
      "height            0.105713\n",
      "total_weight      0.783750\n",
      "meat_weight       0.319148\n",
      "drained_weight    0.149871\n",
      "shell_weight      0.209308\n",
      "num_rings         9.670580\n",
      "dtype: float64\n",
      "Standard Dev length            0.286111\n",
      "width             0.258855\n",
      "height            0.198261\n",
      "total_weight      0.563480\n",
      "meat_weight       0.319780\n",
      "drained_weight    0.216505\n",
      "shell_weight      0.234247\n",
      "num_rings         3.603491\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "means = list(df.loc[:,\"length\":\"num_rings\"].mean())\n",
    "std   = list(df.loc[:,\"length\":\"num_rings\"].std())\n",
    "\n",
    "print(\"Mean \",df.loc[:,\"length\":\"num_rings\"].mean())\n",
    "print(\"Standard Dev\",df.loc[:,\"length\":\"num_rings\"].std())\n",
    "cols = list(df.columns)\n",
    "cols.remove('type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   type  length  width  height  total_weight  meat_weight  drained_weight  \\\n",
      "0   0.0   0.350  0.265   0.090        0.2255       0.0995          0.0485   \n",
      "1   1.0   0.530  0.420   0.135        0.6770       0.2565          0.1415   \n",
      "2   0.0   0.440  0.365   0.125        0.5160       0.2155          0.1140   \n",
      "3   2.0  -1.000  0.255   0.080        0.2050       0.0895          0.0395   \n",
      "4   2.0   0.425  0.300   0.095        0.3515       0.1410          0.0775   \n",
      "\n",
      "   shell_weight  num_rings  length_zscore  width_zscore  height_zscore  \\\n",
      "0         0.070         -1      -0.448112     -0.390757      -0.079254   \n",
      "1         0.210          9       0.181014      0.208034       0.147720   \n",
      "2         0.155         10      -0.133549     -0.004440       0.097282   \n",
      "3         0.055          7      -5.166563     -0.429388      -0.129692   \n",
      "4         0.120          8      -0.185976     -0.255546      -0.054034   \n",
      "\n",
      "   total_weight_zscore  meat_weight_zscore  drained_weight_zscore  \\\n",
      "0            -0.990718           -0.686870              -0.468215   \n",
      "1            -0.189447           -0.195908              -0.038663   \n",
      "2            -0.475172           -0.324121              -0.165681   \n",
      "3            -1.027099           -0.718142              -0.509784   \n",
      "4            -0.767108           -0.557094              -0.334268   \n",
      "\n",
      "   shell_weight_zscore  num_rings_zscore  \n",
      "0            -0.594708         -2.961179  \n",
      "1             0.002952         -0.186092  \n",
      "2            -0.231843          0.091417  \n",
      "3            -0.658743         -0.741109  \n",
      "4            -0.381258         -0.463600  \n"
     ]
    }
   ],
   "source": [
    "for col in cols:\n",
    "    col_zscore = col + '_zscore'\n",
    "    #forula for Z Score\n",
    "    df[col_zscore] = (df[col] - df[col].mean())/df[col].std()\n",
    "    \n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing text (Optional)\n",
    "\n",
    "One possible way to transform text documents into vectors of numeric attributes is to use the TF-IDF representation. We will experiment with this representation using the 20 Newsgroup data set. The data set contains postings on 20 different topics. The classification problem is to decide which of the topics a posting falls into. Here, we will only consider postings about medicine and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of each category is: [(0, 'sci.med'), (1, 'sci.space')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import math\n",
    "\n",
    "categories = ['sci.med', 'sci.space']\n",
    "raw_data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "print('The index of each category is: {}'.format([(i,target) for i,target in enumerate(raw_data.target_names)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out some of the postings, might find some funny ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sci.space email.\n",
      "\n",
      "There are 1187 emails.\n",
      "\n",
      "From: MUNIZB%RWTMS2.decnet@rockwell.com (\"RWTMS2::MUNIZB\")\n",
      "Subject: How do they ignite the SSME?\n",
      "X-Added: Forwarded by Space Digest\n",
      "Organization: [via International Space University]\n",
      "Original-Sender: isu@VACATION.VENARI.CS.CMU.EDU\n",
      "Distribution: sci\n",
      "Lines: 21\n",
      "\n",
      "on Date: Sat, 3 Apr 1993 12:38:50 GMT, Paul Dietz <dietz@cs.rochester.edu>\n",
      "writes:\n",
      "\n",
      "/in essence, holding a match under the nozzle, is just *nuts*.  One\n",
      "/thing you absolutely must do in such an engine is to guarantee that\n",
      "/the propellants ignite as soon as they mix, within milliseconds.  To\n",
      "/do otherwise is to fill your engine with a high explosive mixture\n",
      "/which, when it finally does ignite, blows everything to hell.\n",
      "\n",
      "Definitely! In one of the reports of an early test conducted by Rocketdyne at \n",
      "their Santa Susanna Field Lab (\"the Hill\" above the San Fernando and Simi \n",
      "Valleys), the result of a hung start was described as \"structural failure\" of \n",
      "the combustion chamber.  The inspection picture showed pumps with nothing below\n",
      ", the CC had vaporized!  This was described in a class I took as a \"typical\n",
      "engineering understatement\" :-)\n",
      "\n",
      "Disclaimer: Opinions stated are solely my own (unless I change my mind).\n",
      "Ben Muniz     MUNIZB%RWTMS2.decnet@consrt.rockwell.com    w(818)586-3578\n",
      "Space Station Freedom:Rocketdyne/Rockwell:Structural Loads and Dynamics\n",
      "   \"Man will not fly for fifty years\": Wilbur to Orville Wright, 1901\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "idx = np.random.randint(0, len(raw_data.data))\n",
    "print ('This is a {} email.\\n'.format(raw_data.target_names[raw_data.target[idx]]))\n",
    "print ('There are {} emails.\\n'.format(len(raw_data.data)))\n",
    "print(raw_data.data[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets pick the first 10 postings from each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_med = np.flatnonzero(raw_data.target == 0)\n",
    "idxs_space = np.flatnonzero(raw_data.target == 1)\n",
    "idxs = np.concatenate([idxs_med[:10],idxs_space[:10]])\n",
    "data = np.array(raw_data.data)\n",
    "data = data[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.nltk.org/\">NLTK</a> is a toolkit for natural language processing. Take some time to install it and go through this <a href=\"http://www.slideshare.net/japerk/nltk-in-20-minutes\">short tutorial/presentation</a>.\n",
    "\n",
    "The downloaded package below is a tokenizer that divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pandoora/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in data]\n",
    "vocabulary_size = 1000\n",
    "unknown_token = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1641 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print (\"Found %d unique words tokens.\" % len(word_freq.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 1000.\n",
      "The least frequent word in our vocabulary is 'REASONS' and appeared 1 times.\n"
     ]
    }
   ],
   "source": [
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    " \n",
    "print (\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print (\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "od = collections.OrderedDict(sorted(word_to_index.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "Code your own TF-IDF representation function and use it on this dataset. (Don't use code from libraries. Build your own function with Numpy/Pandas). Use the formular TFIDF = TF * (IDF+1). The effect of adding “1” to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer()\n",
    "df = pd.DataFrame(countvec.fit_transform(data).toarray(), columns=countvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this implementation correct?\n",
      "Answer: Yes\n"
     ]
    }
   ],
   "source": [
    "def tfidf(df):\n",
    "    for col in list(df.columns):\n",
    "        #formula for for idf\n",
    "        df[col] = df[col] * (math.log(20/sum(df[col] != 0)) +1)    \n",
    "    return df\n",
    "\n",
    "rep = tfidf(df)\n",
    "\n",
    "# Check if your implementation is correct\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(norm=None, smooth_idf=False, use_idf=True)\n",
    "X_train = pd.DataFrame(vectorizer.fit_transform(data).toarray(), columns=countvec.get_feature_names())\n",
    "answer=['No','Yes']\n",
    "if rep is not None:\n",
    "    print ('Is this implementation correct?\\nAnswer: {}'.format(answer[1*np.all(X_train == rep)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
